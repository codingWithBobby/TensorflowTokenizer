{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hi guys and welcome back to a new video. In this video I will show you guys how to encode words with tensorflow and kera. There are diffrent ways you could go about doing this but the way i in this video we will be using the tokenizer. There is no setup required for this video. Just make sure that you have tensorflow and keras installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  #first we want to import the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [   #next we are going to put our sentecnes into an array \n",
    "    'i love my dog',\n",
    "    'I, love my dog',\n",
    "    'i love my cat'\n",
    "]\n",
    "#IT IGNORES THE PUNCUATION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So what we want to do here is have our tokenizer encode each unique word in our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love my dog', 'I, love my dog', 'i love my cat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100) #create an instacne of the class\n",
    "#the arguments num_words is the amount of unique words that you have in our dataset. So for our purpose 100 is way to much as we\n",
    "#only have 5 unique words, but if you are using a large dataset chances are that you do not know the number of unique words so\n",
    "#so it is always good to play it safe and set the num_words to a high number\n",
    "tokenizer.fit_on_texts(sentences) #fit_on_texts method takes in the data and encodes it \n",
    "word_index = tokenizer.word_index  #the tokenizer provides us with a word_index property that reurns us a dictionary containing\n",
    "#key value paris, so the key is the word and the value is is the token for that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)  #now we can go ahead and print out our word index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "as we see there we have gotten back a dictionary with all the unique words, all the puncuation has been ignored and each unique words as a number associated with it. So thats it for this video. In this video we have succesfully enoceded our words using tensorflow and keras. Please like, subscribe, and comment some video ideas. Thanks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
